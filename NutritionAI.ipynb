{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3mHAALbg9dPNq6KJpUgSU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Notskx1950/NutritionRecognize/blob/main/NutritionAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "42NtMx6hr0h6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "xkatuet2HA0k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "n0oeizdqHiFU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "# import food dataset\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"dansbecker/food-101\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7YT3U3qut0f",
        "outputId": "3dc6c7c4-db55-483c-dbbd-80b42ffaa1c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dansbecker/food-101?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.38G/9.38G [06:22<00:00, 26.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/dansbecker/food-101/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/root/.cache/kagglehub/datasets/dansbecker/food-101/versions/1/food-101/food-101'\n",
        "batch_size = 32\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "H4CzAwZdASTx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_txt_path = os.path.join(data_dir, \"meta\", \"classes.txt\")\n",
        "with open(classes_txt_path, \"r\") as f:\n",
        "    classes_list = f.read().splitlines()"
      ],
      "metadata": {
        "id": "p6uPIGibCvw9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classes_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEKtc8qqDQwt",
        "outputId": "ae512384-0013-4565-f22c-c6897af77d4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Label_encoder:\n",
        "    def __init__(self, labels):\n",
        "        self.labels = {label: idx for idx, label in enumerate(labels)}\n",
        "    def get_label(self, idx):\n",
        "        return list(self.labels.keys())[idx]\n",
        "    def get_idx(self, label):\n",
        "        return self.labels.get(label)"
      ],
      "metadata": {
        "id": "yY7I3QBhe-UH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_list = classes_list[:20] + ['other']"
      ],
      "metadata": {
        "id": "MaJpPhFIqMTp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(classes_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHNp-hfXEpjm",
        "outputId": "cc2c75b8-636c-4d26-a395-c8dd0a294905"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Label_encoder(classes_list)"
      ],
      "metadata": {
        "id": "C8wvE4BQEsOw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(classes_list)):\n",
        "  print(encoder.get_label(i),encoder.get_idx(encoder.get_label(i)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sekEgII9E1G4",
        "outputId": "1c15c0a2-0b04-4e37-cfbe-dc0e62ada142"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple_pie 0\n",
            "baby_back_ribs 1\n",
            "baklava 2\n",
            "beef_carpaccio 3\n",
            "beef_tartare 4\n",
            "beet_salad 5\n",
            "beignets 6\n",
            "bibimbap 7\n",
            "bread_pudding 8\n",
            "breakfast_burrito 9\n",
            "bruschetta 10\n",
            "caesar_salad 11\n",
            "cannoli 12\n",
            "caprese_salad 13\n",
            "carrot_cake 14\n",
            "ceviche 15\n",
            "cheesecake 16\n",
            "cheese_plate 17\n",
            "chicken_curry 18\n",
            "chicken_quesadilla 19\n",
            "other 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Food101Dataset(Dataset):\n",
        "    def __init__(self, root_dir, split=\"train\", transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to the 'food-101' folder.\n",
        "            split (str): 'train' or 'test'\n",
        "            transform: torchvision transforms to apply.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        # No need to add /\n",
        "        self.images_dir = os.path.join(self.root_dir, \"images\")\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read image identifiers from e.g., \"meta/train.txt\"\n",
        "        split_file = os.path.join(root_dir, \"meta\", f\"{split}.txt\")\n",
        "        with open(split_file, \"r\") as f:\n",
        "            self.samples = f.read().splitlines()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # e.g., \"apple_pie/1005641\"\n",
        "        relative_path = self.samples[idx]\n",
        "        # Split \"apple_pie/1005641\" into class_name = \"apple_pie\", image_id = \"1005641\"\n",
        "        class_name, image_id = relative_path.split('/')\n",
        "        img_path = os.path.join(self.images_dir, class_name,image_id + \".jpg\")\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Get the class index\n",
        "        label = encoder.get_idx(class_name)\n",
        "\n",
        "        # Apply any specified transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "3h9kt1_gFx0K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "bmZIpv2pGoVT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Food101Dataset(root_dir=data_dir, split=\"train\", transform=train_transforms)\n",
        "test_dataset  = Food101Dataset(root_dir=data_dir, split=\"test\", transform=test_transforms)"
      ],
      "metadata": {
        "id": "8IyIcPDlGpX_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_dataset(dataset):\n",
        "    \"\"\"Filters out samples with class names not in the encoder.\"\"\"\n",
        "    filtered_samples = []\n",
        "    for sample in dataset.samples:\n",
        "        class_name, _ = sample.split('/')\n",
        "        if encoder.get_idx(class_name) is not None:\n",
        "            filtered_samples.append(sample)\n",
        "    dataset.samples = filtered_samples\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "tHKpeA2eLZFC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = filter_dataset(train_dataset)\n",
        "test_dataset = filter_dataset(test_dataset)"
      ],
      "metadata": {
        "id": "_3ocYq7KPI0U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "9MA2xRxIHE64"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "num_classes = 21\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqF9INGgIMVR",
        "outputId": "731cffd7-603a-4f30-cfd4-d2ba40581d23"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 126MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "XTvjBYuoIh4L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n9YSWfPeIuwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% || \"\n",
        "          f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "# Save your model\n",
        "torch.save(model.state_dict(), \"food101_resnet18.pth\")\n",
        "print(\"Training finished and model saved.\")"
      ],
      "metadata": {
        "id": "ZSXU0xSSIrn7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}